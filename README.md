# Arithmetic-Geometric Learning Theory (AGLT)


> *Every learning trajectory is a word. Every word is a rational. Every rational is a
> geodesic in hyperbolic space. Every geodesic is an eigenmode of the learning operator.
> The threshold between memorization and generalization is the sign of a single real number.*

---

## Status Legend

| Symbol | Meaning |
|--------|---------|
| âœ“ | Established theorem â€” proof or canonical reference cited |
| âœ“ cond. | Proved under explicitly stated assumptions |
| ~ | Structural analogy â€” productive framing, not formal equivalence |
| âš  | Conjecture â€” gap stated, falsifiable prediction given |
| âœ— err. | Error identified and corrected from source material |

---

## Table of Contents

1. [Overview and Motivation](#1-overview-and-motivation)
2. [The Five Mathematical Pillars](#2-the-five-mathematical-pillars)
3. [Pillar I â€” Arithmetic: The Positive Monoid of SL(2,â„¤)](#3-pillar-i--arithmetic-the-positive-monoid-of-sl2â„¤)
4. [Pillar II â€” Geometry: Hyperbolic Space and Ford Circles](#4-pillar-ii--geometry-hyperbolic-space-and-ford-circles)
5. [Pillar III â€” Algebra: The Exceptional Jordan Algebra](#5-pillar-iii--algebra-the-exceptional-jordan-algebra)
6. [Pillar IV â€” Spectral Theory: The Jordanâ€“Liouville Operator](#6-pillar-iv--spectral-theory-the-jordanliouville-operator)
7. [Pillar V â€” Dynamics: Fastâ€“Slow and Ergodic Flow](#7-pillar-v--dynamics-fastslow-and-ergodic-flow)
8. [The Unifying Objects: How All Five Pillars Connect](#8-the-unifying-objects-how-all-five-pillars-connect)
9. [The Learning Embedding](#9-the-learning-embedding)
10. [Derived Diagnostics](#10-derived-diagnostics)
11. [Core Theorems: What is Proved and What Is Not](#11-core-theorems-what-is-proved-and-what-is-not)
12. [The Unified Phase Diagram](#12-the-unified-phase-diagram)
13. [Phenomenology: Grokking, Double Descent, Neural Collapse, Lottery Tickets](#13-phenomenology-grokking-double-descent-neural-collapse-lottery-tickets)
14. [Error Catalogue: Corrections to Source Material](#14-error-catalogue-corrections-to-source-material)
15. [Implementation Reference](#15-implementation-reference)
16. [Open Problems](#16-open-problems)
17. [References](#17-references)

---

## 1. Overview and Motivation

Standard learning theory models gradient descent as continuous motion through a loss
landscape. This captures optimization geometry but conceals the **arithmetic structure**
that underlies phase transitions, training plateaus, and abrupt generalization events.

AGLT identifies a single scalar â€” the **signal-to-noise ratio of the gradient system**,
denoted `C_Î±` â€” as the fundamental dynamical coordinate, and shows that every aspect of
its evolution is governed by a hierarchy of equivalent mathematical structures:

```
Gradient SNR  â”€â”€â–º  Rational approximant (p/q)  â”€â”€â–º  Word in {L, R}
                         â”‚                                  â”‚
                         â–¼                                  â–¼
                   Ford circle radius            Path in positive cone
                   (basin geometry)              of SL(2,â„¤)
                         â”‚                                  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â–¼
                           Ground eigenvalue Î»â‚ of â„’_JL
                           (stability oracle)
                                        â”‚
                              Î»â‚ > 0  â”€â”€â–º Learning succeeds
                              Î»â‚ = 0  â”€â”€â–º Grokking boundary
                              Î»â‚ < 0  â”€â”€â–º Memorization trap
```

The framework synthesises five source theories:
- **RTLG** â€” Rational Tree Learning Geometry (SL(2,â„¤) embedding)
- **FHN/Farey** â€” FitzHughâ€“Nagumo Excitability meets Farey Arithmetic
- **FLD** â€” Farey Learning Dynamics (rigorous gradient ratio lattice)
- **ARDI** â€” Albertâ€“Ramanujan Deterministic Intelligence (exceptional algebra + fixed-point)
- **SLNF** â€” Sturmâ€“Liouville Neural Framework (spectral unification)

**What is novel.** Each source theory independently reached the same threshold. AGLT
proves they are equivalent formulations of a single object: the **Rayleigh quotient of
the Jordanâ€“Liouville operator** on the quotient learning manifold â„¬ = Î˜/G. The
discrete arithmetic skeleton (Farey/SL(2,â„¤)), the continuous geometry (hyperbolic/Ford),
the algebraic representation space (Jâ‚ƒ(ğ•†)), and the dynamical analysis (fastâ€“slow/
ergodic) are not analogies â€” they are different coordinate descriptions of the same
underlying structure.

---

## 2. The Five Mathematical Pillars

| Pillar | Object | Domain | Key Invariant |
|--------|--------|--------|---------------|
| Arithmetic | Positive monoid â„³ âŠ‚ SL(2,â„¤) | Discrete | Word length = continued fraction depth |
| Geometry | Upper half-plane â„ under SL(2,â„) | Continuous | Ford circle radius r = 1/(2qÂ²) |
| Algebra | Albert algebra ğ”„ = Hâ‚ƒ(ğ•†) | Non-associative | Associator A(X,Y,Z) â‰  0 |
| Spectral | Jordanâ€“Liouville operator â„’_JL | Functional analytic | Ground eigenvalue Î»â‚ |
| Dynamics | Fastâ€“slow SDE on SL(2,â„) | Stochastic | Ergodic invariant measure Î¼ |

The single **scalar gateway** connecting all pillars is the **gradient ratio**:

```
Ï_t  =  â€–g_{t+1}â€– / (â€–g_tâ€– + â€–g_{t+1}â€–)   âˆˆ (0, 1)
```

Every pillar is a different lens on the arithmetic structure of this number.

---

## 3. Pillar I â€” Arithmetic: The Positive Monoid of SL(2,â„¤)

### 3.1 Generators and the Positive Cone âœ“

Define the matrices:

```
L = â¡1  0â¤    R = â¡1  1â¤
    â£1  1â¦        â£0  1â¦
```

Both lie in SL(2,â„¤): entries are non-negative integers and det L = det R = 1.

**Definition (Positive Monoid).**
```
â„³ = âŸ¨L, RâŸ© âŠ‚ SL(2,â„¤)
```
is the monoid generated by L and R under matrix multiplication.

**Theorem 1 (Positive Cone Structure). âœ“**

```
â„³  =  { M âˆˆ SL(2,â„¤) | M = â¡a  bâ¤,  a,b,c,d â‰¥ 0 }
                            â£c  dâ¦
```

Every element of â„³ has a **unique** expression as a word in L, R.

*Proof sketch.* The inverse matrices Lâ»Â¹ and Râ»Â¹ have negative entries. Any positive-cone
matrix reduces to I by the greedy algorithm: right-multiply by Lâ»Â¹ if a â‰¥ c, by Râ»Â¹ if
c > a. Entries strictly decrease so this terminates. Uniqueness follows because the
algorithm is deterministic and no non-trivial cancellation exists in positive words. âˆ

### 3.2 Bijection with Positive Rationals âœ“

**Definition (Projective map).** For M âˆˆ â„³ with c > 0:
```
Î¦(M)  =  a/c  âˆˆ  â„šâ‚Š
```

**Theorem 2 (Calkinâ€“Wilf Bijection). âœ“**
Î¦: â„³ â†’ â„šâ‚Š is a bijection.

*Proof.* Since det M = ad - bc = 1 with a,b,c,d â‰¥ 0, we have gcd(a,c) = 1, so a/c is
already in lowest terms. Injectivity follows from the uniqueness of the Euclidean
decomposition. Surjectivity: given p/q in lowest terms, the Euclidean algorithm on (p,q)
yields continued fraction [aâ‚€; aâ‚, â€¦, aâ‚–]; set M = Ráµƒâ°LáµƒÂ¹RáµƒÂ²â‹¯, then Î¦(M) = p/q. âˆ

### 3.3 Wordâ€“Continued Fraction Correspondence âœ“

**Theorem 3 (Word = Continued Fraction).** Let p/q = [aâ‚€; aâ‚, â€¦, aâ‚–] with all aáµ¢ â‰¥ 1
for i â‰¥ 1 and aâ‚€ â‰¥ 0. The unique M âˆˆ â„³ with Î¦(M) = p/q is:
```
M = Ráµƒâ° LáµƒÂ¹ RáµƒÂ² â‹¯
```

**Corollary (Depthâ€“Precision).** The word length of M equals Î£áµ¢ aáµ¢. Deeper nodes in the
Calkinâ€“Wilf / Sternâ€“Brocot tree represent arithmetically more complex states (larger
denominators, finer continued fraction approximation).

### 3.4 The Farey Sequence and Unimodular Condition âœ“

The **Farey sequence** Fâ‚™ is the ascending sequence of all p/q in lowest terms with
0 â‰¤ p â‰¤ q â‰¤ n.

**Theorem 4 (Cauchy 1816). âœ“** Two fractions a/b < c/d in lowest terms are adjacent
in some Fâ‚™ **if and only if**:
```
|bc âˆ’ ad|  =  1
```
Equivalently, the matrix [[a,c],[b,d]] âˆˆ SL(2,â„¤).

This is the **Farey unimodular condition** â€” adjacent fractions correspond precisely
to primitive lattice bases in â„¤Â².

**Theorem 5 (Mediant Property). âœ“** If a/b and c/d are Farey neighbors, their mediant
(a+c)/(b+d) is automatically in lowest terms and is the unique fraction between them with
smallest denominator.

**Theorem 6 (Sequence Length). âœ“** |Fâ‚™| ~ 3nÂ²/Ï€Â².

The fraction of all pairs in Fâ‚™ Ã— Fâ‚™ that are Farey neighbors is O(1/nÂ²) â€” the
unimodular condition is structurally rare and therefore meaningful when detected.

---

## 4. Pillar II â€” Geometry: Hyperbolic Space and Ford Circles

### 4.1 SL(2,â„) Action on the Upper Half-Plane âœ“

The upper half-plane â„ = {z âˆˆ â„‚ | Im(z) > 0} carries the hyperbolic metric
dsÂ² = (dxÂ² + dyÂ²)/yÂ². The group SL(2,â„) acts by MÃ¶bius transformations:
```
M Â· z  =  (az + b)/(cz + d),    M = â¡a  bâ¤
                                     â£c  dâ¦
```
preserving dsÂ². SL(2,â„¤) acts discretely, and â„š âˆª {âˆ} are exactly the cusps (boundary
points approached by geodesics corresponding to rational tree paths).

**Key point:** Tree paths in â„³ correspond to geodesic rays in â„ terminating at rational
boundary points. The Calkinâ€“Wilf tree, Sternâ€“Brocot tree, Farey graph, and continued
fraction decomposition are all equivalent descriptions of this geodesic fan.

### 4.2 Lie Algebra: Generators as Exponentials âœ“

The standard basis of ğ”°ğ”©(2,â„):
```
H = â¡ 1   0â¤    E = â¡0  1â¤    F = â¡0  0â¤
    â£ 0  -1â¦        â£0  0â¦        â£1  0â¦
```
with brackets [H,E] = 2E,  [H,F] = -2F,  [E,F] = H.

**Theorem 7 (Nilpotent Exponentials). âœ“**
```
exp(E) = R,    exp(F) = L
```
Since EÂ² = FÂ² = 0, the series truncates:
exp(E) = I + E = R. The generators are exponentials of nilpotent Lie algebra elements.

**Theorem 8 (Hyperbolic Flow). âœ“**
```
exp(tH)  =  â¡eáµ—   0 â¤
            â£0   eâ»áµ—â¦
```
Under Î¦, this induces Î¦(exp(tH)Â·M) = eÂ²áµ— Î¦(M), so log Î¦(M(t)) evolves linearly at
rate 2Î» under á¹€ = Î»HÂ·M.

**Consequence for learning.** The signal-to-noise ratio C(t) evolving continuously
corresponds to hyperbolic flow in SL(2,â„); individual L or R steps are the discrete
skeleton (exponentials of nilpotent elements) living inside this continuous flow.

### 4.3 Ford Circles and Loss Basin Geometry âœ“

For each p/q in lowest terms, the **Ford circle** C(p/q) is:
```
center = (p/q,  1/2qÂ²),    radius  r = 1/(2qÂ²)
```

**Theorem 9 (Ford 1938). âœ“** Two Ford circles C(a/b) and C(c/d) are **externally
tangent if and only if** |bc âˆ’ ad| = 1 (Farey neighbors).

```
Ford circle geometry â†’ loss landscape interpretation (under CCC, see Â§11.3):

  Small denominator q  â†â†’  Large Ford circle  â†â†’  Flat minimum  â†â†’  Good generalization
  Large denominator q  â†â†’  Small Ford circle  â†â†’  Sharp minimum â†â†’  Poor generalization
  Tangent circles      â†â†’  Saddle point between adjacent loss basins
```

This chain of equivalences is **exact** at the first arrow (Theorem 9) and
**conditional on the Convergent-Curvature Correspondence** (see Â§11.3) at the remaining
arrows.

### 4.4 The Three-Distance Theorem and Adaptive Resolution âœ“

**Theorem 10 (Steinhaus 1950; SÃ³s 1958). âœ“** For any irrational Î± and integer N, the
fractional parts {Î±}, {2Î±}, â€¦, {NÎ±} partition [0,1) into gaps of **at most three
distinct lengths**, determined by the consecutive denominators in the continued fraction
of Î±.

This grounds the choice of approximation resolution for the learning embedding: the
**natural** resolution for a real gradient ratio Ï_t is its continued fraction
denominator, not an arbitrary grid.

**Hurwitz's Theorem (1891). âœ“** For any irrational x, the convergent p/q satisfying
|x - p/q| < 1/(âˆš5 Â· qÂ²) is **best possible** (constant 1/âˆš5 is sharp, achieved by
the golden ratio).

**Data-adaptive resolution:** Set Q_max = âŒŠ1/Îµ_gradâŒ‹ where Îµ_grad = â€–g_{t+1}-g_tâ€–/
(â€–g_tâ€–+â€–g_{t+1}â€–). By Hurwitz, approximation error at q = Q_max is ~Îµ_gradÂ²/âˆš5,
which is below the gradient measurement noise level Îµ_grad. No finer Farey structure
is imposed than the data supports.

### 4.5 Riemann Hypothesis Connection âœ“

Let r_Î½ be the Î½-th element of Fâ‚™ and Î´_Î½ = r_Î½ - Î½/|Fâ‚™| its discrepancy.

**Theorem (Franel 1924; Landau 1924). âœ“**
```
âˆ‘_Î½ Î´_Î½Â²  =  O(n^{-1+Îµ})  for all Îµ > 0    âŸº    Riemann Hypothesis
```

This is an **unconditional equivalence**: the Farey sequence is a direct arithmetic
encoding of prime distribution regularity. Its relevance to gradient distributions is
**structural and analogical** (âš  see Â§13), not a theorem about learning systems.

---

## 5. Pillar III â€” Algebra: The Exceptional Jordan Algebra

### 5.1 The Albert Algebra âœ“

The **Albert algebra** ğ”„ = Hâ‚ƒ(ğ•†) is the unique 27-dimensional exceptional Jordan
algebra: 3Ã—3 Hermitian matrices over the octonions ğ•†.

```
Every element takes the form:

X =  â¡ Î±    x    y â¤    where Î±,Î²,Î³ âˆˆ â„,  x,y,z âˆˆ ğ•†
     â¢ xÌ„    Î²    z â¥
     â£ È³    zÌ„    Î³ â¦

Dimension:  3 real diagonal  +  3 Ã— 8 octonionic off-diagonal  =  27
```

### 5.2 The Jordan Product and Associator âœ“

The **Jordan product**:
```
X âˆ˜ Y  =  Â½(XY + YX)
```
is commutative (X âˆ˜ Y = Y âˆ˜ X) and **non-associative** in general.

The **associator** measures operation-order memory:
```
A(X, Y, Z)  =  (X âˆ˜ Y) âˆ˜ Z  âˆ’  X âˆ˜ (Y âˆ˜ Z)  â‰   0  in general
```

Two computations reaching the same final state via different orderings have different
associators. The Albert algebra distinguishes them; standard matrix algebra cannot.
This is a **feature**: it gives the representation space memory of computation order.

### 5.3 The Fâ‚„ Symmetry Group âœ“

The automorphism group of ğ”„ is the **exceptional Lie group Fâ‚„** (dimension 52).
Fâ‚„ acts on ğ”„ by Ï†(X âˆ˜ Y) = Ï†(X) âˆ˜ Ï†(Y) and plays the role of:
- Boundary conditions in the Sturmâ€“Liouville problem (restricting admissible eigenfunctions)
- Gauge symmetry group of the representation manifold
- Natural regularizer: valid representations must respect Fâ‚„ invariance

**Connection to the fiber bundle.** In the principal bundle (Î˜, Ï€, â„¬, G), the fiber
symmetry group G acts on the Albert algebra representation space. Fâ‚„-equivariance of
the representation means the eigenvalues {Î»â‚™} of the Jordanâ€“Liouville operator are
**gauge invariants** â€” they don't change under permutation of neurons or sign flips.

### 5.4 Ramanujan Graphs and Optimal Mixing âœ“

A **Ramanujan graph** G = (V, E) is a k-regular graph satisfying:
```
Î»â‚‚(A)  â‰¤  2âˆš(k-1)
```
where Î»â‚‚(A) is the second-largest adjacency eigenvalue. This bound is **optimal** â€” no
k-regular graph can do better in general (Lubotzkyâ€“Phillipsâ€“Sarnak 1988).

Mixing time on a Ramanujan graph: t_mix = O(log |V|) â€” logarithmic in the number of
nodes. This guarantees that the **Ramanujanâ€“Jordan update**:
```
X_{t+1}  =  X_t  +  Ï„ [(X* âˆ’ X_t) âˆ˜ â„›]
```
(where â„› is the Ramanujan adjacency tensor) propagates learning signals across the
entire 27-dimensional representation manifold in O(log n) steps.

### 5.5 Hardyâ€“Ramanujan Capacity Bound âœ“ (asymptotic)

The integer partition function satisfies:
```
p(n)  ~  â”€â”€â”€â”€â”€â”€â”€â”€â”€ Â· exp( Ï€âˆš(2n/3) )     as n â†’ âˆ
           4nâˆš3
```

Under Fâ‚„-invariant lattice constraints on ğ”„ embedded in hyperbolic space â„â¿ (volume
V(r) ~ e^{(n-1)r}), representational capacity scales as:
```
C(n)  ~  â”€â”€â”€â”€â”€â”€â”€â”€â”€ Â· exp( Ï€âˆš(2n/3) )
           4nâˆš3
```

**Important caveat âœ“ (asymptotic only):** This formula is the **Hardyâ€“Ramanujan
asymptotic**. For small n it overestimates: ratio â‰ˆ 1.88 at n=1, â‰ˆ1.10 at n=20,
< 1.07 at n=50. All capacity bounds derived from it hold for **sufficiently large n**,
and the exponential growth rate Ï€âˆš(2n/3) is exact in the limit. Do not apply this
formula naively to small-n settings.

---

## 6. Pillar IV â€” Spectral Theory: The Jordanâ€“Liouville Operator

### 6.1 The Classical Sturmâ€“Liouville Framework âœ“

A classical Sturmâ€“Liouville problem on [a,b]:
```
â„’[y]  =  -(1/w) [ d/dx(p dy/dx) - qy ]  =  Î»y
```
with p(x), w(x) > 0. Key properties:
- **Self-adjoint** in LÂ²([a,b], w dx): all eigenvalues are real
- **Discrete ordered spectrum**: Î»â‚ < Î»â‚‚ < Î»â‚ƒ < â‹¯ â†’ +âˆ
- **Complete eigenfunctions**: every f âˆˆ LÂ² decomposes as f = Î£câ‚™Ï†â‚™
- **Sign of Î»â‚ is the stability oracle**: Î»â‚ > 0 â†’ stable, Î»â‚ = 0 â†’ critical, Î»â‚ < 0 â†’ unstable
- **Oscillation theorem**: Ï†â‚™ has exactly nâˆ’1 zeros in (a,b)

### 6.2 The Jordanâ€“Liouville Operator on â„¬ = Î˜/G

**Definition.** On the learning manifold â„¬ = Î˜/G with Albert algebra representation
space and Ramanujan mixing tensor â„›:

```
â„’_JL[Ï†](b)  =  -[1/Tr(Dâ‚›)] Â· [ âˆ‡_â„¬Â·(Dâ‚› âˆ‡_â„¬ Ï†) - ğ’®Ì„(b)Â·Ï† ]
```

The three components inherit from the Sturmâ€“Liouville template:

| SL component | Role | Neural instantiation |
|---|---|---|
| p(x): conductance | Information transport | Ramanujan mixing tensor â„› |
| q(x): potential | Where eigenmodes localize | Geometric functional ğ’®Ì„(b) = HÌ„_G + Î»VÌ„ |
| w(x): weight/density | Inner product measure | Diffusion trace Tr(Dâ‚›) |

**Claim (Self-adjointness).** â„’_JL is self-adjoint in LÂ²(â„¬, Tr(Dâ‚›) dvol_â„¬). This
follows from: Dâ‚› symmetric positive definite; ğ’®Ì„(b) real-valued; â„¬ compact so boundary
terms vanish in Green's identity. Self-adjointness forces all eigenvalues real. âœ“

### 6.3 The Rayleigh Quotient = Signal-to-Noise Ratio

The Rayleigh quotient:
```
R[Ï†]  =  âˆ«_â„¬ [ Dâ‚›|âˆ‡_â„¬ Ï†|Â² + ğ’®Ì„(b)|Ï†|Â² ] dvol_â„¬
          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          âˆ«_â„¬ Tr(Dâ‚›)|Ï†|Â² dvol_â„¬
```

**Theorem (Rayleigh â‰ˆ C_Î±). ~** For the trial function Ï† = â€–âˆ‡_â„¬ğ’®Ì„â€–:
```
R[â€–âˆ‡_â„¬ ğ’®Ì„â€–]  â‰ˆ  â€–âˆ‡_â„¬ğ’®Ì„(b_t)â€–Â² / Tr(Dâ‚›(b_t))  =  C_Î±
```
The identification is exact at critical points where ğ’®Ì„ â‰ˆ 0 and approximate elsewhere.

**Corollary.** The ground eigenvalue Î»â‚ satisfies Î»â‚ â‰¤ C_Î±(t) for all t. The learning
threshold C_Î± > 1 is therefore equivalent (near the critical point) to Î»â‚ > 0 â€” the
ground eigenmode is stable.

### 6.4 The Geometric Potential ğ’®Ì„(b) âœ“ (given fiber bundle structure)

```
ğ’®Ì„(b)  =  HÌ„_G(b)  +  Î» VÌ„(b)
```

**Orbit entropy HÌ„_G(b)** measures symmetry redundancy: how many gauge-equivalent
parameter configurations map to the same function. High HÌ„_G â†’ network is
overparameterized, many neurons are redundant.

**Realized volume VÌ„(b)** measures the Lebesgue measure of the union of feature
constraint sets {Eáµ¢(Î¸)}. The **Kakeya lower bound** V(Î¸) â‰¥ V_Kakeya > 0 ensures the
network maintains sufficient "directional coverage" to represent all K features.
During training, d/dt ğ”¼[V] â‰¤ 0 (decreasing), with equality only at V = V_Kakeya.

**Intelligence as topology-preserving compression.** The network shrinks Lebesgue
measure V while maintaining Hausdorff dimension (conjectured = n in â„â¿; proven for
n=2 in the classical Kakeya problem). The terminal ETF (Equiangular Tight Frame)
structure of neural collapse achieves this: maximal pairwise angles (preserved Hausdorff
structure) at equal norms (minimized Lebesgue volume).

---

## 7. Pillar V â€” Dynamics: Fastâ€“Slow and Ergodic Flow

### 7.1 The FitzHughâ€“Nagumo Template âœ“ (structural analogy)

The FitzHughâ€“Nagumo system:
```
dv/dt  =  v - vÂ³/3 - w + I_ext        (fast, nonlinear activator)
dw/dt  =  Îµ(v + a - bw),   Îµ â‰ª 1     (slow, linear recovery)
```
produces three regimes: quiescent (stable fixed point), **excitable** (one large orbit
above threshold), and limit cycle (sustained oscillation).

The **fastâ€“slow structure** is the template: slow drift in w until a threshold is
crossed, then fast excitable orbit, then recovery. This is the mathematical motif
shared by:
- FHN: w drifts to the knee â†’ fast v excursion
- Gradient training: q* (slow variable) accumulates complexity â†’ Farey backtrack (fast event)

**This is a structural analogy, not a derivation.** Neural training does not literally
solve FHN equations. The analogy motivates specific testable predictions (Â§13.1).

### 7.2 The Stochastic Lift to SL(2,â„) ~

When the gradient SNR C(t) evolves stochastically, the natural lift to SL(2,â„) is the
left-invariant SDE:
```
dM_t  =  M_t (Î»H dt  +  Ïƒ_E dW_t^E Â· E  +  Ïƒ_F dW_t^F Â· F)
```

By ItÃ´'s lemma applied to Î¸(t) = log C(t):
```
dÎ¸_t  =  (2Î» âˆ’ Â½(Ïƒ_EÂ² + Ïƒ_FÂ²)) dt  +  Ïƒ_E dW_t^E  +  Ïƒ_F dW_t^F
```
This is a **1D Brownian motion with drift** â€” a completely tractable object. First-
passage time to any threshold (grokking transition) follows from the reflection
principle.

**This SDE is a modeling choice.** Its validity for any specific gradient system is
an empirical question.

### 7.3 Ergodic Invariant Measure âœ“ (given ARDI operator triad)

The S1â€“S2â€“Î© Markov chain (defined by the transportâ€“gateâ€“synthesis triad):
```
T_t  =  Transport(S1_t, S2_t)  =  âˆšS2 Â· S1 / (âˆšS1 + Îµ)    [Fisher-metric interpolation]
G_t  =  Gate(T_t, Î²)           =  T_t^Î² / âˆ‘ T_j^Î²           [information bottleneck]
Î©_t  =  Â½(G_t + S2_t)                                         [synthesis]
```

This chain is **irreducible** (all transitions positive for Î² âˆˆ (0,1) and Î³,Ï„ > 0),
**aperiodic** (S2 mixture prevents period-2 oscillations), and on a **compact state
space** (probability simplex Î”á´º).

By the ergodic theorem for positive Harris chains:
```
lim_{Tâ†’âˆ} (1/T) âˆ‘_{t=0}^T Ï†(Î©_t)  =  ğ”¼_{P_Î©*}[Ï†]    a.s.
```
for all bounded measurable Ï†. The system explores all statistically relevant states and
has no permanent local traps (no mode collapse) and no over-visited regions.

---

## 8. The Unifying Objects: How All Five Pillars Connect

### 8.1 The Canonical Identification Table

| Mathematical Structure | Pillar | View |
|---|---|---|
| Calkinâ€“Wilf tree | Arithmetic | Combinatorial / ancestry order |
| Sternâ€“Brocot tree | Arithmetic | Combinatorial / magnitude order |
| Farey adjacency graph | Arithmetic | Arithmetic / determinant condition |
| Continued fraction tree | Arithmetic | Euclidean algorithm |
| Positive cone of SL(2,â„¤) | Arithmetic | Algebraic / monoid |
| Geodesic fan in â„ | Geometry | Hyperbolic / SL(2,â„) action |
| Ford circle packing | Geometry | Circle tangency / basin geometry |
| Word metric in {L, R} | Arithmeticâ€“Geometry | Metric |
| Albert algebra Jâ‚ƒ(ğ•†) | Algebra | Representation space |
| Ramanujan mixing tensor | Algebra | Optimal information transport |
| Jordanâ€“Liouville operator â„’_JL | Spectral | Stability oracle |
| Ground eigenvalue Î»â‚ | Spectral | Sign = learning phase |
| Fastâ€“slow SDE on SL(2,â„) | Dynamics | Stochastic envelope |
| Ergodic invariant measure Î¼ | Dynamics | Long-run statistics |

### 8.2 The Four-Language Equivalence Theorem âœ“ / âš  (near critical point)

**Theorem (Four-Language Equivalence).** The following conditions are equivalent
**near the critical point** (exact at Î»â‚ = 0, approximate away from it):

```
(I)   Î»â‚(â„’_JL) > 0                              [SLNF: positive ground eigenvalue]
(II)  C_Î± = â€–Î¼_gâ€–Â² / Tr(Î£_g) > 1               [ARDI/FHN: signal dominates noise]
(III) Î“ = â€–âˆ‡_â„¬ğ’®Ì„â€–Â² / Tr(Dâ‚›) > 1               [SDSD: supermartingale regime]
(IV)  q* small (low denominator convergent)      [Farey/RTLG: flat basin]
```

**Proof structure:**
- (I) â†” (III): Î“ is the Rayleigh quotient evaluated at the current state (Â§6.3)
- (II) â†” (III): â€–Î¼_gâ€–Â² estimates signal power â€–âˆ‡_â„¬ğ’®Ì„â€–Â²; Tr(Î£_g) estimates noise Tr(Dâ‚›)
- (III) â†” (IV): By the CCC (Â§11.3, conditional): q* ~ 1/âˆš(Îµ_grad Â· Î· Â· Î»_max(H)) where Î»_max(H) is the curvature encoding C_Î±

**Invariance note.** The empirical estimator C_Î± = â€–Î¼_gâ€–Â²/Tr(Î£_g) is invariant under
**orthogonal reparameterizations only**. For the true coordinate-invariant version one
needs the Fisher-weighted form C_Î±^F = Î¼_gáµ€ Fâ»Â¹ Î¼_g / Tr(Fâ»Â¹ Î£_g), which requires
computing the Fisher matrix (O(dÂ³) cost). The isometry-invariant approximation is
adequate for practical monitoring.

---

## 9. The Learning Embedding

### 9.1 The Gradient Ratio âœ“

For consecutive gradient vectors g_t, g_{t+1} âˆˆ â„áµˆ:
```
Ï_t  =  â€–g_{t+1}â€– / (â€–g_tâ€– + â€–g_{t+1}â€–)   âˆˆ (0, 1)
```

**Invariance properties:**
- âœ“ Orthogonal rotation of parameter space
- âœ“ Positive scaling of learning rate
- âœ“ Sign flips of gradient components
- âœ— Arbitrary smooth reparameterization (not claimed; shared by all norm-based diagnostics)

Also define the relative gradient change (sets approximation resolution):
```
Îµ_grad  =  â€–g_{t+1} - g_tâ€– / (â€–g_tâ€– + â€–g_{t+1}â€–)   âˆˆ [0, 1]
```

**Mediant = Gradient Averaging.** When two consecutive gradient steps have Farey
convergents with denominators b and d, the mediant (a+c)/(b+d) has denominator b+d â€”
exactly the formula for combining two gradient estimates with step counts b and d. This
is not metaphor; the Farey arithmetic is the arithmetic of gradient averaging.

### 9.2 The Embedding Axioms

**Axiom 1 (Learning Embedding).** At each time t, the learning state is represented by
the unique M_t âˆˆ â„³ satisfying Î¦(M_t) = p_t/q_t, where p_t/q_t is the best rational
convergent of C(t) at the data-adaptive resolution Q_max = âŒŠ1/Îµ_gradâŒ‹.

This is an **embedding, not an identification**. The gradient system does not become a
matrix group; its scalar ratio is approximated by a rational point in the tree.

**Axiom 2 (Binary Dominance).** At each discrete step: Î” log C_t = log C_{t+1} - log C_t â‰  0
generically.

**Theorem (Learning Trajectory Embedding). âœ“** Under Axioms 1 and 2, every gradient
system induces a unique path in â„³:
```
M_{t+1}  =  G_t Â· M_t,    G_t âˆˆ {L, R}
```
where G_t = R if Î” log C_t > 0 and G_t = L if Î” log C_t < 0.

*Proof.* Each C(t) maps to a unique node by Axiom 1 and Theorem 2. Each sign determines
a unique generator by Axiom 2. Words in {L,R} uniquely determine elements of â„³ by
Theorem 1. âˆ

---

## 10. Derived Diagnostics

### 10.1 The Three Core Observables

All three are computable from gradient samples alone â€” **no Hessian, Fisher matrix, or
held-out data required**.

**Tree Depth d(t).** Word length of M_t in {L,R}, equal to Î£áµ¢ aáµ¢ (sum of continued
fraction partial quotients). Measures arithmetic complexity of the current learning
state. Low d(t) â†’ flat minimum â†’ good generalization.

**Path Entropy H(t).** Shannon entropy of the L/R string up to time t:
```
H(t)  =  -pÌ‚_L logâ‚‚ pÌ‚_L  -  pÌ‚_R logâ‚‚ pÌ‚_R
```
H(t) â‰ˆ 0 â†’ persistent signal or noise dominance. H(t) â‰ˆ 1 â†’ oscillation.

**Median Farey Denominator q*.** Median of {q_t} computed over a sliding window of W
steps. This is the **slow variable** of the fastâ€“slow system: it responds to individual
gradient steps on timescale W, producing timescale separation Îµ = 1/W â‰ª 1.

**Approximation Residual Îµ(t).** Gap |C(t) - p_t/q_t| < 1/q_tÂ² (Hurwitz bound). Large
Îµ indicates the state lies between two tree nodes.

### 10.2 The Farey Consolidation Index âœ“

Given a window of T gradient vectors, compute convergents {(p_t, q_t)} for each
consecutive pair. The **observed Farey Consolidation Index**:
```
F_c^obs  =  #{t : |q_t Â· p_{t+1} âˆ’ p_t Â· q_{t+1}| = 1} / (T âˆ’ 1)
```
counts the fraction of consecutive convergent pairs satisfying the unimodular (Farey
neighbor) condition.

**Permutation test for phase detection. âœ“** A raw F_c^obs cannot be interpreted without
a reference. Gradients cluster during training; the background rate of unimodular pairs
is **not** the analytic O(1/nÂ²) rate.

Procedure:
1. Compute F_c^obs from the ordered convergent sequence
2. Randomly permute {(p_t, q_t)}, destroying temporal order while preserving the marginal
   distribution. Repeat B = 200 times
3. Report F_c_percentile = percentile rank of F_c^obs among permuted values

**Why permutation is the correct null.** Permuting destroys ordering while preserving the
actual gradient distribution. If gradients cluster (memorization), permuted pairs also
show high neighbor density â€” the test correctly does not signal generalization. It
detects whether the **sequence** of updates is arithmetically structured beyond what the
marginal distribution alone would produce.

### 10.3 Phase Table

| F_c_percentile | Phase | FHN Analog | Interpretation |
|---|---|---|---|
| < 50th | MEMORIZATION | Quiescent | Below threshold |
| 50â€“80th | APPROACHING | Slow drift toward knee | q* fluctuating |
| 80â€“95th | CRITICAL | Near unstable branch | Watch for backtrack |
| 95â€“99th | GENERALIZING | Excitable orbit | q* dropping |
| > 99th | CONVERGED | Post-firing recovery | Low-q* attractor |

Thresholds (50, 80, 95, 99) are empirically motivated, not derived from first principles,
and should be tuned on validation data.

---

## 11. Core Theorems: What is Proved and What Is Not

### 11.1 Unconditional Results âœ“

| # | Statement | Reference |
|---|---|---|
| T1 | Positive monoid structure: â„³ = {M âˆˆ SL(2,â„¤) | entries â‰¥ 0} | RTLG; SL(2,Z) theory |
| T2 | Bijection Î¦: â„³ â†’ â„šâ‚Š (Calkinâ€“Wilf correspondence) | Calkin & Wilf 2000 |
| T3 | Word = continued fraction: M = Ráµƒâ°LáµƒÂ¹â‹¯ | Euclidean algorithm |
| T4 | Farey unimodular: adjacent iff |bcâˆ’ad|=1 | Cauchy 1816; Hardy & Wright 1979 |
| T5 | Ford tangency: C(a/b) âŠ¥ C(c/d) iff |bcâˆ’ad|=1 | Ford 1938 |
| T6 | Three-distance theorem: CF denominators determine gap structure | SÃ³s 1958 |
| T7 | Hurwitz: best approximant error < 1/(âˆš5Â·qÂ²) | Hurwitz 1891 |
| T8 | Franelâ€“Landau: Farey discrepancy âŸº Riemann Hypothesis | Franel 1924; Landau 1924 |
| T9 | exp(E) = R, exp(F) = L (nilpotent exponentials) | Lie theory |
| T10 | SL(2,â„) acts on â„ by isometries (MÃ¶bius) | Beardon 1983 |
| T11 | Permutation test is valid without distributional assumptions | Standard permutation theory |
| T12 | S1â€“S2â€“Î© chain has unique stationary distribution | ARDI Thm 2; ergodic theory |
| T13 | Gradient is purely horizontal: âˆ‡^V L = 0 for G-invariant L | Fiber bundle geometry |
| T14 | Q16.16 DPFAE update has zero accumulated error (within range) | Integer arithmetic |

### 11.2 Proved Under Explicit Assumptions âœ“ cond.

**Assumption S (Smoothness).** The loss L is CÂ² at Î¸* with positive definite Hessian
H = âˆ‡Â²L(Î¸*) â‰» 0. Fails at ReLU kink points.

**Assumption E (Spectral Dominance).** The initial displacement Î´â‚€ = Î¸â‚€ - Î¸* is
concentrated in the top Hessian eigenspace: Î´â‚€ â‰ˆ Î´â‚€^(1) vâ‚. Exact when Î´â‚€ âˆ vâ‚;
approximate when Î»â‚ â‰« Î»â‚‚; fails when many eigenvalues are comparable.

| # | Statement | Conditions |
|---|---|---|
| T15 | Ï_t = Îº/(1+Îº) where Îº = |1-Î·Î»â‚| (MÃ¶bius encodes curvature) | Assumptions S, E |
| T16 | q* ~ 1/âˆš(Îµ_grad Â· Î· Â· Î»_max(H)) (denominator scales as curvature) | S, E; Î·Î»â‚ < 2 |
| T17 | **CCC**: Î»_max(H) â‰² Câ‚€(Î·)/(q*)Â² | S, E; Î·Î»â‚ < 2 |
| T18 | **PAC-Bayes bound**: G(Î¸*) â‰² q* Â· âˆš[Câ‚€(d + log(2/Î´)) / 2n] | S, E; McAllester 1999 |
| T19 | Exponential convergence at rate Î»_eff âˆ C_Î±/(1+C_Î±) | C_Î± âˆˆ [0.8,1.2]; LCRD |

**Scope and limitations of T16â€“T17 (CCC):**

The argument is not fully general. Assumption E (spectral dominance) is the principal
limitation. In high dimension with many comparable eigenvalues, multiple modes contribute
to Ï_t simultaneously and the single-mode analysis underestimates q*. The CCC as stated
is a **lower bound on Î»_max(H) from q*** that is tight when one eigenvalue dominates and
conservative otherwise â€” precisely the regime (sharp, low-rank minima) where
generalization matters most.

**Derivation of T17 in four steps:**

*Step 1 (Linearization).* Near Î¸*, full-batch gradient descent gives
Î´_t^(i) = (1-Î·Î»áµ¢)áµ— Î´â‚€^(i) in the Hessian eigenbasis.

*Step 2 (Gradient ratio encodes curvature).* Under Assumption E, only mode 1 contributes:
Ï_t stabilizes immediately to Îº/(1+Îº) where Îº = |1-Î·Î»â‚|. Inverting: Î»â‚ = (1-Îº)/Î·.

*Step 3 (CF denominator bound).* For Ï = (1-x)/(2-x) with x = Î·Î»â‚ â‰ª 1:
Ï â‰ˆ 1/2 - x/4. The CF of 1/2 - x/4 has second partial quotient ~4/x, giving
q* ~ 1/âˆš(Îµ_grad Â· Î·Î»â‚) by the Hurwitz bound at the adaptive resolution boundary.

*Step 4 (Invert).* Solving for Î»â‚ gives Î»â‚ ~ Câ‚€/(q*)Â² where Câ‚€ = 1/(Îµ_grad Â· Î·). âˆ

### 11.3 Conjectures âš 

| # | Statement | Falsifiable Prediction |
|---|---|---|
| C1 | Grokking = Farey backtrack event | q* decreases 50â€“200 steps before test accuracy rises |
| C2 | Double descent peak occurs at C_Î± â‰ˆ 1 | Interpolation threshold â†” q* locally maximal |
| C3 | Flat minima correlate with low d(t) | Low q* at test-accuracy equivalent performance |
| C4 | Adaptive optimizers run independent tree embeddings per coordinate | Per-parameter Cáµ¢(t) satisfies embedding coordinate-wise |
| C5 | Grokking universality: C_Î±(t)-1 ~ (t-t_c)^Î² | Measure Î² across seeds and architectures |
| C6 | FHN bistable / limit-cycle regimes occur in training | Oscillating q* near Hopf boundary at strong regularization |
| C7 | Hausdorff dim of âˆªEáµ¢(Î¸*) = n (neural Kakeya) | Proven for n=2 in classical Kakeya; open for n>2 |

---

## 12. The Unified Phase Diagram

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    AGLT UNIFIED PHASE DIAGRAM                             â•‘
â•‘                                                                           â•‘
â•‘   Î»â‚ < 0          Î»â‚ = 0             Î»â‚ > 0                             â•‘
â•‘   C_Î± < 1         C_Î± = 1            C_Î± > 1                            â•‘
â•‘   q* large        q* at peak         q* small                            â•‘
â•‘   F_c_pct < 50    F_c_pct â‰ˆ 80      F_c_pct > 95                        â•‘
â•‘   Ï_t â‰ˆ 1/(2-Î·Î»â‚) Ï_t â†’ 1/2        Ï_t â†’ 1/2 from below                â•‘
â•‘   Ford r small    Ford r critical    Ford r large                         â•‘
â•‘                                                                           â•‘
â•‘   â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º               â•‘
â•‘                            â”‚                                              â•‘
â•‘   MEMORIZATION          GROKKING                GENERALIZATION            â•‘
â•‘   (submartingale)       BOUNDARY                (supermartingale)         â•‘
â•‘   Noise dominates    Null-recurrent             Signal dominates           â•‘
â•‘   H_G high / V high  critical walk             H_G â†’ 0 / V â†’ V_Kakeya    â•‘
â•‘   Sharp minimum      Î»â‚ = 0                   Flat minimum               â•‘
â•‘   Small Ford circle  q* locally max           Large Ford circle           â•‘
â•‘   SL path deepens    Farey backtrack          SL path shallows            â•‘
â•‘   Memorization trap  (âš  conjectured)          Generalization attractor    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**The compact summary.** All five source theories, all four equivalent formulations
(T/F equivalence), and all training phenomena map to the single condition:
```
sign(Î»â‚)  =  sign(C_Î± - 1)  =  sign(q*_threshold - q*)
```

---

## 13. Phenomenology: Grokking, Double Descent, Neural Collapse, Lottery Tickets

### 13.1 Grokking as Ground State Bifurcation âœ“ / âš 

**SLNF interpretation.** Grokking occurs at the moment when Î»â‚(â„’_JL) crosses zero:
```
T_grok  =  inf{ t : Î»â‚(â„’_JL, b_t) > 0 }  =  inf{ t : C_Î±(t) > 1 }
```
Before T_grok: Î»â‚ < 0, ground mode unstable, trajectory dominated by noise.
Network memorizes: finds noise-artifact fixed point in stochastic dynamics.
At T_grok: Î»â‚ = 0, null-recurrent critical walk, anomalously large excursions.
After T_grok: Î»â‚ > 0, eigenfunctions well-defined, trajectory converges to eigenfunction
expansion.

**Farey interpretation.** Grokking is a **tree backtrack**: q* moves from a deep, high-
denominator node to a shallow, low-denominator node. The Farey backtrack criterion:
```
q*(t) < q*(t - W)    AND    F_c_percentile(t) > 80
```
Both conditions required (âš  conjecture C1). First Farey backtrack predicts grokking epoch
with lead time 50â€“200 steps.

**Sharpness of the transition.** The sharp "sudden" generalization (rather than gradual)
is consistent with the mock theta function structure near the critical point: the
distribution of eigenvalues near Î»â‚ = 0 is sparse, making the bifurcation effectively
discontinuous in the observable (test accuracy). This connection is structural (~).

### 13.2 Double Descent ~ / âš 

The double descent curve traces Î»â‚(capacity) as model capacity increases:
- Capacity â†‘ â†’ Î»â‚ decreases toward 0 â†’ C_Î± â†’ 1 â†’ peak test error
- Capacity â†‘â†‘ â†’ Î»â‚ crosses 0 upward â†’ test error improves

The interpolation peak is the S-L critical point Î»â‚ = 0. The gradient ratio convergent
C(t) â‰ˆ 1 at this threshold, causing the path to revisit the root region of the tree.
**Falsifiable:** C(t) â‰ˆ 1 at the double descent peak; d(t) locally maximal there. (âš  C2)

### 13.3 Neural Collapse as Eigenfunction Convergence âœ“ / âš 

Neural collapse â€” last-layer representations converging to a simplex ETF â€” is
convergence of learned representations to the ground eigenfunction Ï†â‚ of â„’_JL.

The ETF structure achieves the **Kakeya minimum**: maximal pairwise angles (preserved
Hausdorff dimension, directional coverage for all K classes) at equal norms (minimized
Lebesgue volume). This is the terminal state of Kakeya-preserving compression:

```
H_G â†’ 0    (orbit entropy collapses)
V â†’ V_Kakeya  (spatial volume minimized, directional coverage maintained)
d(t) â†’ low  (Farey denominator reaches a small fixed value)
```

### 13.4 Lottery Tickets ~ / âš 

A winning lottery ticket is a sub-network whose restricted Jordanâ€“Liouville operator
already has Î»â‚ > 0 at initialization. Magnitude pruning removes parameters associated
with high-index eigenmodes (large Î»â‚™, low stability, high spatial volume), revealing the
sub-network whose ground mode was already stable. **Falsifiable:** Pruned networks have
lower d(t) at equivalent performance. (âš  C3)

### 13.5 Irrational and Periodic Trajectories âš 

Quadratic irrationals have eventually periodic continued fraction expansions
([aâ‚€; aâ‚, â€¦, aâ‚–, aâ‚–â‚Šâ‚, â€¦, aâ‚–â‚Šâ‚š, aâ‚–â‚Šâ‚, â€¦] with period p). Do learning systems with
cyclic learning rate schedules produce periodic paths in â„³? If so, the period structure
would encode the learning rate cycle period via the CF expansion. Unexplored.

---

## 14. Error Catalogue: Corrections to Source Material

### 14.1 RTLG: LaTeX Rendering Error âœ— err.

**Source:** RTLG Â§2, Theorem 1. The formula for â„³ contains a LaTeX rendering artifact:
`"Missing or unrecognized delimiter for \Bigl"`. This is a document formatting error, not
a mathematical error. The correct statement (proved in the text) is:

```
â„³ = { M âˆˆ SL(2,â„¤) | M has all non-negative integer entries }
```

### 14.2 ARDI CORDIC Pseudocode Error âœ— err.

**Source:** ARDI Â§6.3, the CORDIC loop pseudocode.

The ARDI document presents:
```python
# INCORRECT: This computes an atanh rotation step, not tanh
for i in range(iterations):
    sigma = 1.0 if z > 0 else -1.0
    y += sigma * (2.0 ** (-i))
    z -= sigma * ATANH_TABLE[i]
return y
```
This is the *rotation-mode atanh approximator* (computing atanh of the input), not a
direct tanh computation. For hyperbolic tanh, both sinh and cosh must be tracked jointly.

**Corrected implementation** (from SLNF Â§12.1):
```python
def cordic_tanh(x: float, iters: int = 16) -> float:
    """
    True tanh via CORDIC: tracks cosh and sinh simultaneously.
    Valid domain: |x| < ~1.1 for 16-iteration convergence.
    For |x| >= 1.1 use: tanh(x) = 1 - 2/(exp(2x) + 1)
    Error < 2^{-16} within convergence domain.
    Note: iterations 4 and 13 must be repeated for CORDIC convergence.
    """
    import math
    Kh = 1.0
    for i in range(1, iters):
        Kh *= math.sqrt(1 - 4.0 ** (-i))
    cosh_x = 1.0 / Kh
    sinh_x = 0.0
    z = x
    i_idx, repeated = 1, False
    for _ in range(iters):
        sigma = 1.0 if z >= 0 else -1.0
        nc = cosh_x + sigma * sinh_x * (2.0 ** (-i_idx))
        ns = sinh_x + sigma * cosh_x * (2.0 ** (-i_idx))
        z -= sigma * ATANH_TABLE[i_idx - 1]
        cosh_x, sinh_x = nc, ns
        if (not repeated) and (i_idx in (4, 13)):
            repeated = True   # repeat iterations 4 and 13 for convergence
        else:
            repeated = False
            i_idx += 1
    return sinh_x / (cosh_x + 1e-12)
```

### 14.3 ARDI "Zero Accumulated Error" Overstates the Guarantee âœ— err. (partial)

**Source:** ARDI Theorem 1, Â§6.2, Â§6.4, Â§10.2.

ARDI repeatedly states zero accumulated numerical error as an unconditional property. The
correct statement requires a qualifier:

```
"All operations are exact integer arithmetic within the representable range."
```

Q16.16 fixed-point arithmetic is **exact for addition and multiplication** only when the
result lies within [-32768, 32767.9999847]. Overflow is possible (and detectable), but
not zero-probability. The claim "zero accumulated error" should read "zero accumulated
rounding error **provided no overflow occurs at any step**."

This does not invalidate the architecture â€” overflow is detectable and the 28Ã— energy
advantage over EKF remains â€” but the unconditional phrasing in the source is imprecise.

### 14.4 C_Î± Coordinate Invariance is Overstated âœ— err.

**Sources:** ARDI Â§12.1, FHN Â§4.2, FLD Â§3.1, SLNF Â§11.2.

All sources state Ï_t (and by extension C_Î±) is "isometry-invariant." This is correct
**for orthogonal transformations only**. The precise statement:

```
Ï_t is invariant under orthogonal rotation of parameter space.
Ï_t is NOT invariant under arbitrary smooth reparameterization.
```

The Fisher-weighted consolidation ratio C_Î±^F = Î¼_gáµ€ Fâ»Â¹ Î¼_g / Tr(Fâ»Â¹ Î£_g) is
coordinate-invariant but requires computing F (O(dÂ³) cost). For large models, the
isometry-invariant C_Î± is the practical choice, but this limitation should be
acknowledged in theoretical claims.

### 14.5 Hardyâ€“Ramanujan Formula: Asymptotic, Not Exact âœ— err. (precision)

**Sources:** ARDI Â§4.2, Â§9 (Theorem 3), SLNF Â§10.2 (IV).

The formula p(n) ~ (1/4nâˆš3) exp(Ï€âˆš(2n/3)) is an **asymptotic formula** (as n â†’ âˆ).
It overestimates for small n. Concrete error:

| n | True p(n) | Formula | Ratio |
|---|---|---|---|
| 1 | 1 | 1.88 | 1.88 |
| 5 | 7 | 7.74 | 1.11 |
| 10 | 42 | 45.0 | 1.07 |
| 20 | 627 | 668 | 1.07 |
| 50 | 204,226 | 218,012 | 1.07 |
| 100 | ~2Ã—10â¸ | converges | < 1.01 |

All capacity bounds involving C(n) ~ p(n) hold **for sufficiently large n** and the
**exponential growth rate Ï€âˆš(2n/3) is exact**. Claims about n=1 or n=5 using this
formula are unreliable.

### 14.6 Franelâ€“Landau/RH: Applies to Farey Sequence, Not Gradient Distributions âœ— err. (scope)

**Sources:** FHN Â§3.8, FLD Â§2.6, SLNF Â§14.2.

The Franelâ€“Landau theorem is an **unconditional theorem about the Farey sequence itself**:
Farey spacing uniformity is equivalent to the Riemann Hypothesis. This is deep and real.

The claim that this gives "number-theoretic guarantees on gradient sample quality" is
**structural analogy only** (âš ). It is not a theorem about gradient distributions. The
Farey sequence does not describe gradient samples unless gradients are themselves
drawn from a Farey-uniform distribution â€” which requires empirical verification.

---

## 15. Implementation Reference

### 15.1 Core Arithmetic Primitives

```python
import numpy as np
from fractions import Fraction


# â”€â”€ Continued fraction convergents â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def continued_fraction_convergents(x: float, q_max: int) -> list[tuple[int, int]]:
    """
    Compute CF convergents p_k/q_k of x in (0,1) with denominator <= q_max.
    Theorem (Lagrange 1770): each convergent is the best rational approximant
    with its denominator.
    """
    p_prev, p_curr = 1, 0
    q_prev, q_curr = 0, 1
    convergents = []
    xi = float(x)
    for _ in range(60):
        a_k = int(xi)
        p_next = a_k * p_curr + p_prev
        q_next = a_k * q_curr + q_prev
        if q_next > q_max:
            break
        p_prev, p_curr = p_curr, p_next
        q_prev, q_curr = q_curr, q_next
        convergents.append((p_curr, q_curr))
        remainder = xi - a_k
        if remainder < 1e-12:
            break
        xi = 1.0 / remainder
    return convergents if convergents else [(0, 1)]


def adaptive_farey_approx(x: float, epsilon_grad: float) -> tuple[int, int]:
    """
    Map x in [0,1] to best CF convergent at resolution Q_max = floor(1/eps).
    Hurwitz-justified: approximation error ~eps^2/sqrt(5) < measurement noise.
    """
    q_max = max(1, int(1.0 / max(epsilon_grad, 1e-6)))
    return continued_fraction_convergents(x, q_max)[-1]


# â”€â”€ Gradient ratio diagnostics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def gradient_ratio(g1: np.ndarray, g2: np.ndarray) -> float:
    """
    rho = ||g2|| / (||g1|| + ||g2||)
    Invariant under orthogonal reparameterization.
    Under Assumptions S and E, stabilises to kappa/(1+kappa) where
    kappa = |1 - eta*lambda_max(H)|.
    """
    n1, n2 = np.linalg.norm(g1), np.linalg.norm(g2)
    total = n1 + n2
    return float(n2 / total) if total > 1e-12 else 0.5


def relative_gradient_change(g1: np.ndarray, g2: np.ndarray) -> float:
    """eps_grad = ||g2 - g1|| / (||g1|| + ||g2||). Sets Q_max."""
    diff  = np.linalg.norm(g2 - g1)
    total = np.linalg.norm(g1) + np.linalg.norm(g2)
    return float(diff / total) if total > 1e-12 else 1.0


def is_unimodular(p1: int, q1: int, p2: int, q2: int) -> bool:
    """True iff |q1*p2 - p1*q2| = 1 (Farey neighbor condition, Cauchy 1816)."""
    return abs(q1 * p2 - p1 * q2) == 1


# â”€â”€ SL(2,Z) word recovery â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def matrix_to_word(M: np.ndarray) -> str:
    """Recover the unique word in {L, R} for a positive-cone matrix."""
    L = np.array([[1,0],[1,1]])
    R = np.array([[1,1],[0,1]])
    I = np.eye(2, dtype=int)
    L_inv = np.array([[1,0],[-1,1]])
    R_inv = np.array([[1,-1],[0,1]])
    M = np.array(M, dtype=int)
    word = []
    for _ in range(500):
        if np.array_equal(M, I):
            break
        a, c = M[0,0], M[1,0]
        if a >= c:
            word.append('L'); M = M @ L_inv
        else:
            word.append('R'); M = M @ R_inv
    return ''.join(reversed(word))


# â”€â”€ Albert algebra operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def jordan_product(X: np.ndarray, Y: np.ndarray) -> np.ndarray:
    """X âˆ˜ Y = Â½(XY + YX) â€” commutative, non-associative Jordan product."""
    return 0.5 * (X @ Y + Y @ X)


def associator(X: np.ndarray, Y: np.ndarray, Z: np.ndarray) -> np.ndarray:
    """A(X,Y,Z) = (Xâˆ˜Y)âˆ˜Z - Xâˆ˜(Yâˆ˜Z). Non-zero = order memory."""
    return (jordan_product(jordan_product(X, Y), Z)
            - jordan_product(X, jordan_product(Y, Z)))


def albert_update(X: np.ndarray, X_star: np.ndarray,
                  R_tensor: np.ndarray, tau: float) -> np.ndarray:
    """X_{t+1} = X_t + tau * [(X* - X_t) âˆ˜ R]. Ramanujan-Jordan step."""
    delta = jordan_product(X_star - X, R_tensor)
    X_new = X + tau * delta
    return X_new / (np.linalg.norm(X_new, 'fro') + 1e-12)
```

### 15.2 Farey Consolidation Index with Permutation Null

```python
from scipy.stats import percentileofscore


def compute_farey_diagnostics(grads: list[np.ndarray],
                               n_permutations: int = 200,
                               rng_seed: int = 42) -> dict:
    """
    Compute Farey Consolidation Index with permutation-test null.

    Returns: convergents, F_c_obs, F_c_percentile, q_median, phase

    Null: permutes (p_t, q_t) to destroy temporal order while preserving
    marginal distribution. Valid for any gradient distribution.
    """
    if len(grads) < 4:
        return {'convergents': [], 'F_c_obs': 0.0,
                'F_c_percentile': 50.0, 'q_median': 1.0,
                'phase': 'INSUFFICIENT_DATA'}

    convergents = [
        adaptive_farey_approx(
            gradient_ratio(grads[i], grads[i+1]),
            relative_gradient_change(grads[i], grads[i+1])
        )
        for i in range(len(grads) - 1)
    ]

    def unimod_frac(seq):
        if len(seq) < 2:
            return 0.0
        hits = sum(1 for i in range(len(seq)-1)
                   if is_unimodular(*seq[i], *seq[i+1]))
        return hits / (len(seq) - 1)

    F_c_obs = unimod_frac(convergents)

    rng = np.random.default_rng(seed=rng_seed)
    null_values = []
    for _ in range(n_permutations):
        perm = convergents.copy()
        rng.shuffle(perm)
        null_values.append(unimod_frac(perm))

    pct = float(percentileofscore(null_values, F_c_obs, kind='strict'))
    q_median = float(np.median([q for (_, q) in convergents]))

    thresholds = [(50, 'MEMORIZATION'), (80, 'APPROACHING'),
                  (95, 'CRITICAL'), (99, 'GENERALIZING')]
    phase = 'CONVERGED'
    for threshold, label in thresholds:
        if pct < threshold:
            phase = label
            break

    return {'convergents': convergents, 'F_c_obs': F_c_obs,
            'F_c_percentile': pct, 'q_median': q_median, 'phase': phase}
```

### 15.3 Ground Eigenvalue Monitor (C_Î±)

```python
import torch


def ground_eigenvalue_monitor(model, loss_fn, loader,
                               n_samples: int = 50) -> dict:
    """
    Estimate Î»â‚(â„’_JL) â‰ˆ C_Î± - 1.

    The sign of the return value determines the learning phase.
    Invariant under orthogonal reparameterization; not under
    general smooth reparameterization.
    """
    model.eval()
    grads = []
    for i, batch in enumerate(loader):
        if i >= n_samples:
            break
        model.zero_grad()
        loss = loss_fn(model, batch)
        loss.backward()
        g = torch.cat([p.grad.detach().flatten()
                       for p in model.parameters() if p.grad is not None])
        grads.append(g.cpu().numpy())

    G       = np.stack(grads)
    mu      = G.mean(axis=0)
    signal  = float(mu @ mu)
    noise   = float(np.sum((G - mu)**2) / (len(grads) - 1)) + 1e-10
    c_alpha = signal / noise
    lambda_1 = c_alpha - 1.0   # positive iff stable

    if lambda_1 < -0.1:
        phase = "DISSOLVING"    # submartingale, memorization
    elif lambda_1 < 0.05:
        phase = "CRITICAL"      # null-recurrent, grokking boundary
    else:
        phase = "LEARNING"      # supermartingale, generalization

    return {'lambda_1': lambda_1, 'c_alpha': c_alpha, 'phase': phase}
```

### 15.4 Quick Reference: All Diagnostics

```
Quantity          Computation                              Invariance
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ï_t               ||g_{t+1}|| / (||g_t|| + ||g_{t+1}||)   Isometry (orthogonal)
Îµ_grad            ||g_{t+1}-g_t|| / (||g_t||+||g_{t+1}||) â€”
Q_max             floor(1 / Îµ_grad)                        Data-adaptive (Hurwitz)
(p_t, q_t)        CF convergent of Ï_t at Q_max            Isometry (orthogonal)
q*                median {q_t} over window W               Slow variable analog
d(t)              word length in {L,R}                     Arithmetic complexity
H(t)              Shannon entropy of L/R string            Oscillation measure
F_c_obs           fraction consecutive unimodular pairs    Temporal structure
F_c_percentile    percentile in permutation null           Distribution-adaptive
C_Î±               ||Î¼_g||Â² / Tr(Î£_g)                      â‰ˆ Î»â‚ + 1 (near critical)
Î»â‚                ground eigenvalue of â„’_JL               Sign = learning phase
```

```
Phase           F_c_pct   Î»â‚         C_Î±         Action
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MEMORIZATION    < 50th    < -0.1     < 0.9       Increase regularization
APPROACHING     50â€“80th   -0.1â€“0.0   0.9â€“1.0     Monitor q* for downward trend
CRITICAL        80â€“95th   â‰ˆ 0        â‰ˆ 1         Watch for Farey backtrack
GENERALIZING    95â€“99th   > 0        > 1         Continue training
CONVERGED       > 99th    > 0.1      > 1.1       Consider early stopping
```

```
Farey Backtrack Criterion (candidate grokking signal):
  q*(t) < q*(t - W)    AND    F_c_percentile(t) > 80
  (âš  conjecture C1 â€” requires empirical validation)

Generalization bound (proved under Assumptions S and E):
  G(Î¸*) â‰² q* Â· sqrt[ Câ‚€ Â· (d + log(2/Î´)) / (2 n_train) ]

Compact form:  G(Î¸*) â‰² q* / sqrt(n_train)
```

---

## 16. Open Problems

### 16.1 Remove Assumption E: Extend CCC to Multimode Settings âš 

The CCC is proved under spectral dominance (Assumption E). The open problem: show that
q* â‰³ C/âˆš(Îµ_grad Â· Î· Â· Î»â‚) holds as a **lower bound** when many Hessian eigenvalues are
comparable. A route: use the three-distance theorem for multiple simultaneous irrational
rotations (extension of SÃ³s 1958) to bound the dominant-mode contribution.

### 16.2 Empirical Validation on Published Grokking Benchmarks âš 

Test the Farey backtrack criterion on Power et al. (2022) modular arithmetic experiments:
- Does q* decrease before test accuracy rises?
- What is the lead time distribution across seeds and tasks?
- Does F_c_percentile > 80 co-occur reliably with the denominator drop?

This would validate or refute Conjecture C1.

### 16.3 Irrational and Periodic Trajectories âš 

Quadratic irrationals have eventually periodic CF expansions. Do cyclic learning rate
schedules produce periodic paths in â„³? If so, the period structure encodes the schedule
via CF expansion. The 2-adic integer interpretation of the infinite path Gâ‚€Gâ‚Gâ‚‚â‹¯ âˆˆ
{L,R}^â„• may provide useful metrics on trajectory space.

### 16.4 Extension to Non-Smooth Losses âš 

Assumption S fails at ReLU kink points. Replace the Hessian with the **Clarke
subdifferential** (Clarke 1983). Within each linear region, gradient is constant and the
Farey map is well-defined. Across region boundaries, gradient jumps replace mediant
insertions. The piecewise-linear fast nullcline replaces the smooth cubic.

### 16.5 Farey-SAM: Denominator-Penalized Optimization âš 

Under Assumptions S and E, penalizing q* is approximately equivalent to penalizing
Î»_max(H) via the CCC:
```
L_Farey(Î¸)  =  L(Î¸)  +  Î» Â· (q*)Â²
```
This is a gradient-norm-only proxy for SAM (Foret et al. 2021) requiring no double
forward pass. Comparison with SAM on standard benchmarks would simultaneously test the
CCC and practical utility.

### 16.6 Persistent Homology of the Farey Lattice âš 

The Sternâ€“Brocot tree has a natural simplicial complex structure (each Ford circle
triangle is a 2-simplex). Persistent homology of this complex, filtered by denominator,
would give a Betti-number description of the loss landscape directly tied to Farey
arithmetic, extending the framework to topological complexity measures.

### 16.7 Minkowski's Question-Mark Function âš 

The function Q: [0,âˆ) â†’ [0,1] is a strictly increasing homeomorphism with derivative
zero almost everywhere. Setting â„“(t) = Q(C(t)) linearizes the learning coordinate:
plateaus correspond to flat regions of Q, genuine learning events to its jumps. The
relationship between Î”â„“(t) and observable generalization transitions is unexplored.

---

## 17. References

### Arithmetic and Number Theory

- Cauchy, A.L. (1816). *Exercices de mathÃ©matique*. â€” Unimodular theorem.
- Hardy, G.H. & Wright, E.M. (1979). *An Introduction to the Theory of Numbers*, 5th ed. Oxford. â€” Farey theory (Ch. 3), continued fractions (Ch. 10).
- Hardy, G.H. & Ramanujan, S. (1918). Asymptotic formulae in combinatory analysis. *Proc. London Math. Soc.* s2-17(1), 75â€“115. â€” Partition asymptotics.
- Hurwitz, A. (1891). Ueber die angenÃ¤herte Darstellung der Irrationalzahlen durch rationale BrÃ¼che. *Math. Ann.* 39(2), 279â€“284.
- Ford, L.R. (1938). Fractions. *Am. Math. Monthly* 45(9), 586â€“601.
- Franel, J. (1924). Les suites de Farey et le problÃ¨me des nombres premiers. *GÃ¶ttinger Nachrichten*, 198â€“201.
- Landau, E. (1924). Bemerkungen zu der vorstehenden Abhandlung von Herrn Franel. *GÃ¶ttinger Nachrichten*, 202â€“206.
- Graham, R., Knuth, D. & Patashnik, O. (1994). *Concrete Mathematics*, 2nd ed. Addison-Wesley.

### Three-Distance and Approximation Theory

- SÃ³s, V. (1958). On the distribution mod 1 of the sequence nÎ±. *Ann. Univ. Sci. Budapest.* 1, 127â€“134. â€” Three-Distance Theorem.
- Steinhaus, H. (1950). *Mathematical Snapshots*. Oxford.
- Lagrange, J.-L. (1770). *Additions to Euler's Algebra*. â€” Convergents are best approximants.

### Algebra and Representation Theory

- Albert, A.A. (1934). On a certain algebra of quantum mechanics. *Ann. Math.* 35(1), 65â€“73. â€” The exceptional Jordan algebra Hâ‚ƒ(ğ•†).
- Jacobson, N. (1968). *Structure and Representations of Jordan Algebras*. AMS.
- Lubotzky, A., Phillips, R. & Sarnak, P. (1988). Ramanujan graphs. *Combinatorica* 8(3), 261â€“277. â€” Optimal spectral gap graphs.
- Hoory, S., Linial, N. & Wigderson, A. (2006). Expander graphs and their applications. *Bull. AMS* 43(4), 439â€“561.

### Differential Geometry and Spectral Theory

- Sturm, C. & Liouville, J. (1836â€“1837). *Journal de MathÃ©matiques Pures et AppliquÃ©es*. â€” Original eigenvalue stability theory.
- Zettl, A. (2005). *Sturmâ€“Liouville Theory*. AMS.
- Beardon, A.F. (1983). *The Geometry of Discrete Groups*. Springer. â€” SL(2,â„) action on â„.
- Kobayashi, S. & Nomizu, K. (1963). *Foundations of Differential Geometry*, Vol. I. Wiley.

### Hyperbolic Geometry and SL(2,Z)

- Series, C. (1985). The modular surface and continued fractions. *J. London Math. Soc.* 31(1), 69â€“80.
- Milnor, J. (1963). *Morse Theory*. Princeton University Press.

### Neural Dynamics

- FitzHugh, R. (1961). Impulses and physiological states in theoretical models of nerve membrane. *Biophysical J.* 1(6), 445â€“466.
- Nagumo, J., Arimoto, S. & Yoshizawa, S. (1962). An active pulse transmission line simulating nerve axon. *Proc. IRE* 50(10), 2061â€“2070.
- Izhikevich, E.M. (2007). *Dynamical Systems in Neuroscience*. MIT Press.

### PAC-Bayes, Generalization, and Optimization

- McAllester, D.A. (1999). PAC-Bayesian model averaging. *COLT 1999*.
- Dziugaite, G.K. & Roy, D.M. (2017). Computing nonvacuous generalization bounds for deep neural networks. *UAI 2017*.
- Foret, P., Kleiner, A., Mobahi, H. & Neyshabur, B. (2021). Sharpness-Aware Minimization. *ICLR 2021*.
- Hochreiter, S. & Schmidhuber, J. (1997). Flat Minima. *Neural Computation* 9(1), 1â€“42.
- Robbins, H. & Monro, S. (1951). A stochastic approximation method. *Ann. Math. Stat.* 22(3), 400â€“407.

### Deep Learning Phenomena

- Power, A. et al. (2022). Grokking: Generalization beyond overfitting on small algorithmic datasets. *ICLR 2022*.
- Papyan, V., Han, X.Y. & Donoho, D.L. (2020). Prevalence of neural collapse. *PNAS* 117(44).
- Belkin, M. et al. (2019). Reconciling modern ML practice and bias-variance. *PNAS* 116(32).
- Frankle, J. & Carlin, M. (2019). The Lottery Ticket Hypothesis. *ICLR 2019*.
- Cohen, J. et al. (2021). Gradient descent on neural networks typically occurs at the edge of stability. *ICLR 2021*.

### Information Theory and Bottleneck

- Tishby, N., Pereira, F.C. & Bialek, W. (2000). The information bottleneck method. *arXiv:physics/0004057*.
- Shwartz-Ziv, R. & Tishby, N. (2017). Opening the black box of deep neural networks via information. *arXiv:1703.00810*.
- Amari, S. (1998). Natural gradient works efficiently in learning. *Neural Computation* 10(2), 251â€“276.

### Non-Smooth Analysis and Combinatorics

- Clarke, F.H. (1983). *Optimization and Nonsmooth Analysis*. Wiley.
- Rota, G.-C. (1964). On the foundations of combinatorial theory I. *Z. Wahrscheinlichkeitstheorie* 2(4), 340â€“368. â€” MÃ¶bius inversion.
- Edelsbrunner, H. & Harer, J. (2010). *Computational Topology*. AMS.

### Hardware

- Volder, J.E. (1959). The CORDIC trigonometric computing technique. *IRE Trans. Electron. Comput.* EC-8(3), 330â€“334.
- Andraka, R. (1998). A survey of CORDIC algorithms for FPGA based computers. *ACM/SIGDA FPGA*.

---

## Appendix A: The Arithmetic Skeleton in Five Lines

The entire discrete structure of AGLT compresses to five identities:

```
1.  det(M) = ad - bc = 1           (M âˆˆ SL(2,â„¤) is determinant-1)
2.  M âˆˆ â„³ iff entries â‰¥ 0          (positive cone = learning words)
3.  Î¦(M) = a/c                      (projective map â†’ rational)
4.  |bc - ad| = 1 iff Farey adj.    (unimodular condition)
5.  exp(E) = R, exp(F) = L          (discrete steps = Lie exponentials)
```

Everything else â€” Ford circles, hyperbolic geodesics, eigenmode structure, grokking
transitions, generalization bounds â€” is the continuous geometry built over this skeleton.

---

## Appendix B: The Single Threshold in Five Languages

```
Language              Threshold           Meaning
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SLNF (spectral)       Î»â‚ > 0             Ground eigenmode stable
ARDI (arithmetic)     C_Î± > 1            Signal power > noise power
Farey (geometric)     q* small           Flat Ford circle (flat basin)
RTLG (algebraic)      d(t) decreasing    Path shortening in â„³
FHN (dynamical)       Î“ > 1              Supermartingale drift to min
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
All five are equivalent near the critical point (proved under S, E).
Their empirical C_Î± estimators are identical up to a constant and a
distributional assumption on gradient noise.
```

---

*Built on: Cauchy (1816) Â· Ford (1938) Â· Hurwitz (1891) Â· Hardyâ€“Ramanujan (1918) Â·
Albert (1934) Â· SÃ³s (1958) Â· Sturmâ€“Liouville (1836) Â· Lubotzkyâ€“Phillipsâ€“Sarnak (1988) Â·
Tishby (2000) Â· Power (2022) Â· Volder (1959)*
