# SALT â€” Spectral-Arithmetic Learning Theory

> *"Does arithmetic structure precede grokking under fixed-point quantization constraints?"*


### Why software results are invalid for this question:

Every execution path â€” NumPy, SciPy, PyTorch, a Q.16 emulator, a CORDIC
emulator â€” runs on a CPU or GPU. Those processors execute in **IEEE 754 float64 arithmetic**.

```
What you think you are running:
  Q.16 fixed-point training â†’ Q.16 CORDIC log â†’ Q.16 gradient â†’ T_arith

What you are actually running:
  float64 simulation of Q.16 â†’ float64 simulation of CORDIC â†’ float64 gradient â†’ T_arith

These are not the same thing.
```

The SALT observables are all affected:

| Observable | Why float64 â‰  fixed-point silicon |
|---|---|
| `C_Î±` (power-law slope) | Computed via `log` â€” on hardware this is a CORDIC approximation with finite iteration error. float64 `log` is exact to 15 decimal places. The power-law slope is a *different function* on real hardware. |
| `entropy H` | Shannon entropy requires `pÂ·log(p)`. CORDIC log error is non-uniform across the probability range â€” small probabilities are computed less accurately. float64 has no such structure. |
| `denominator` (FFT) | Fixed-point FFT has a quantization noise floor that creates spurious spectral peaks. float64 FFT does not. The dominant frequency â€” and therefore T_arith â€” will differ. |
| `T_grok` | Grokking onset is sensitive to the loss landscape. Q.16 weight updates truncate/saturate every step, permanently altering the trajectory. float64 updates are lossless. |
| `Î”T = T_grok âˆ’ T_arith` | Both endpoints are wrong. The difference is doubly wrong. |

### The emulator trap

It is tempting to write a Q.16 emulator in Python:

```python
def q16_multiply(a, b):
    result = (int(a * 65536) * int(b * 65536)) >> 16
    return max(-32768, min(32767, result))  # saturate
```

This is still running on float64 hardware. The integer arithmetic is simulated inside a
float64 register. The saturation is a conditional branch, not an electrical clamp. The
rounding is a software rule, not a transistor behavior. The accumulator width is Python's
arbitrary-precision integer, not a 32-bit or 40-bit hardware accumulator.

A CORDIC emulator has the same problem:

```
CORDIC emulator runs on  â†’  CPU/GPU  â†’  which is float64 under the hood
Q.16 emulator runs on    â†’  CPU/GPU  â†’  which is float64 under the hood

You are simulating fixed-point behavior using floating-point arithmetic.
It is not the same thing.
```

The quantization noise in real silicon is **structural** â€” it emerges from the physical
bit-width of the datapath and propagates deterministically through every operation. A
software simulation adds noise as a post-hoc approximation. The two noise processes have
different statistical properties, different frequency content, and different effects on
gradient dynamics.

### What valid results require

To answer the research question with scientific validity:

| Requirement | Why |
|---|---|
| **FPGA or ASIC with Q.16 datapath** | Bit-identical arithmetic to target hardware |
| **CORDIC core** (hardware, not emulated) | Softmax, entropy, log â€” all need CORDIC on fixed-point substrate |
| **Fixed-point BLAS / LAGREBA** | Matrix multiply and gradient accumulation at correct accumulator width |
| **Cycle-accurate logging** | T_grok and T_arith must be measured at hardware clock resolution |
| **Bit-identical software reference** | For debugging only â€” not for results |

Minimum viable hardware: **Xilinx Ultrascale+ or Intel Agilex FPGA** with a custom or
HLS-generated Q.16 MAC array and a CORDIC core at 16â€“24 iterations.

### Recommended path

The only valid path to answer this question is a **university lab partnership**
with an ECE or computer architecture group that has:

- FPGA or ASIC infrastructure
- Fixed-point toolchains (Vivado HLS, Cadence, Synopsys)
- Experience with neuromorphic or efficient ML hardware


---

## Summary

| Question | Can this code answer it? |
|---|---|
| Does SALT hold in float32? | Yes â€” run `--mode full` |
| Does SALT hold on Q.16 hardware? | **No. Requires physical fixed-point silicon.** |
| Does a Q.16 emulator answer the hardware question? | **No. Emulators run on float64 CPUs/GPUs.** |
| What would valid results require? | FPGA/ASIC lab, CORDIC core, fixed-point BLAS, hardware logging |
| What is the right next step? | University ECE/architecture lab partnership |

---

## Quick Start

```bash
# Run all 7 demo scenarios (saves plots to ./salt_demo_output/)
python salt_demo.py

# Run a specific scenario
python salt_demo.py --scenario grokking
python salt_demo.py --scenario jordan
python salt_demo.py --scenario live --show

# Run the 64-test suite
python salt_tests.py
```

**Dependencies:** `numpy`, `scipy`, `matplotlib` â€” no GPU, no deep learning framework required.

---

## Files

| File | Description |
|------|-------------|
| `salt_core.py` | Core library â€” all observables, diagnostics, Jordan layer, spectral basis |
| `salt_tests.py` | 64-test suite â€” unit, integration, theorem verification |
| `salt_demo.py` | 7 demo scenarios with full visualizations |

---

## The Central Idea

Every gradient step encodes one of two arithmetic operations:

```
R = [[1,1],[0,1]]   (gradient norm increased)
L = [[1,0],[1,1]]   (gradient norm decreased)
```

These are elements of a free monoid â„³ inside SL(2,â„¤). The word built over training â€” `RLLRRLRR...` â€” is the model's **arithmetic state**. The continued-fraction denominators of the gradient ratio Ï_t measure the curvature of the loss basin the model currently occupies.

Five independent mathematical frameworks all reduce to a single observable, **C_Î±**:

```
C_Î±  =  â€–Î¼_gâ€–Â² / Tr(Î£_g)       gradient signal-to-noise ratio

C_Î± > 1   â†’   signal dominates   â†’   GENERALIZING
C_Î± â‰ˆ 1   â†’   critical boundary  â†’   grokking / double-descent
C_Î± < 1   â†’   noise dominates    â†’   MEMORIZING
```

Computable from gradients alone â€” no Hessian, no held-out data.

---

## Five Pillars

### I â€” Arithmetic

The gradient ratio Ï_t = â€–g_{t+1}â€– / (â€–g_tâ€– + â€–g_{t+1}â€–) is approximated by its best continued-fraction convergent p_t/q_t at data-adaptive precision. Each training step is L or R in the Sternâ€“Brocot tree. Consecutive steps satisfying the **Farey unimodular condition** |q_tÂ·p_{t+1} - p_tÂ·q_{t+1}| = 1 are arithmetically adjacent â€” their Ford circles touch.

### II â€” Geometry

The learning trajectory is a geodesic ray in the upper half-plane â„. Each rational convergent p/q corresponds to a **Ford circle** of radius 1/(2qÂ²). Large radius = flat loss basin = good generalization.

```
small q   â†â†’   large Ford circle   â†â†’   flat basin   â†â†’   generalization
large q   â†â†’   small Ford circle   â†â†’   sharp basin  â†â†’   memorization
```

The generators R and L are exponentials of nilpotent Lie algebra elements: exp(E) = R, exp(F) = L. The arithmetic skeleton lives inside the continuous Lie structure.

### III â€” Algebra

The **Jordan product** replaces standard matrix multiplication:

```python
X âˆ˜ W = Â½(XW + WX)
```

- **Commutative:** Xâˆ˜W = Wâˆ˜X
- **Not associative:** (Xâˆ˜Y)âˆ˜Z â‰  Xâˆ˜(Yâˆ˜Z) â€” the algebra retains memory of computation order
- **Bounded:** â€–Xâˆ˜Wâ€– â‰¤ â€–Xâ€–Â·â€–Wâ€–

This enforces symmetric bilinear feature mixing and acts as an implicit regularizer. Implementable in any linear algebra library.

### IV â€” Spectral Theory

On the quotient learning manifold â„¬ = Î˜/G, define the Jordanâ€“Liouville operator:

```
â„’_JL Ïˆ = -d/dx[p(x) dÏˆ/dx] + q(x)Ïˆ
```

Because â„’_JL is self-adjoint, its eigenvalues are real. The **ground eigenvalue Î»â‚** is the stability oracle:

```
sign(Î»â‚)  =  sign(C_Î± - 1)
```

Its eigenfunctions form an orthonormal LÂ² basis â€” universal approximation guaranteed.

### V â€” Dynamics

Two timescales operate simultaneously:
- **Fast** (per step): individual L/R gradient steps
- **Slow** (window W): evolution of C_Î± and q* (median CF denominator)

The slow variable q* is a noisy, non-monotone proxy for basin curvature â€” use it statistically over windows, not as a pointwise oracle. C_Î± is the primary phase indicator.

---

## Diagnostics at a Glance

All returned by `SaltAnalyzer.step(g_prev, g_curr)`:

| Field | Description | Interpretation |
|-------|-------------|----------------|
| `rho` | Gradient ratio Ï_t âˆˆ (0,1) | Near 0.5 = flat basin |
| `c_alpha` | Signal-to-noise C_Î± | **Primary phase indicator** |
| `q_star` | Median CF denominator | Slow variable; statistical proxy |
| `f_c_pct` | Farey Consolidation Index % | Corroborating signal |
| `path_entropy` | L/R word entropy H âˆˆ [0,1] | 0=persistent trend, 1=plateau |
| `phase` | Training phase label | See below |
| `word` | Full L/R word | Full arithmetic state |

**Phase labels:**

| Phase | C_Î± | Meaning |
|-------|-----|---------|
| MEMORIZATION | < 0.9 | Noise dominates |
| APPROACHING | 0.9â€“1.0 | Transition building |
| CRITICAL | â‰ˆ 1.0 | Grokking boundary / double-descent peak |
| GENERALIZING | > 1.1 | Signal dominates |
| CONVERGED | > 2.0 + FCI | Stable generalization |

---

## Usage

### Attach to any training loop

```python
from salt_core import SaltAnalyzer

analyzer = SaltAnalyzer(window=50)
prev_grad = None

for step in training_loop:
    loss.backward()
    curr_grad = get_flat_gradients(model)   # numpy array shape (d,)

    if prev_grad is not None:
        result = analyzer.step(prev_grad, curr_grad)
        if step % 50 == 0:
            print(f"step={step:4d}  phase={result.phase:<14}  "
                  f"C_Î±={result.c_alpha:.3f}  q*={result.q_star:.0f}")

    prev_grad = curr_grad.copy()
    optimizer.step()
```

### Jordan product layer (numpy)

```python
from salt_core import JordanLayerNumpy

layer = JordanLayerNumpy(dim=64)
output = layer.forward(X)    # X âˆ˜ W = Â½(XW + WX)
```

### Jordan product layer (PyTorch, if available)

```python
from salt_core import JordanLayer   # None if torch not installed
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(128, 64),
    JordanLayer(64),         # symmetric bilinear interaction
    nn.ReLU(),
    nn.Linear(64, 10),
)
```

### Monitor Cauchy convergence

```python
from salt_core import CauchyMonitor

monitor = CauchyMonitor(tol=1e-4, window=20)
for theta in parameter_sequence:
    monitor.update(theta)

print(monitor.is_cauchy())   # True when â€–Î¸_{t+1} - Î¸_tâ€– < tol
print(monitor.trend())       # 'decreasing', 'stable', or 'increasing'
print(monitor.summary())     # {'is_cauchy': bool, 'mean_diff': float, ...}
```

### Spectral features

```python
from salt_core import (
    sturm_liouville_eigenfunctions,
    spectral_features,
    spectral_reconstruct,
)

x, psi = sturm_liouville_eigenfunctions(n_modes=20, n_points=256)
coeffs = spectral_features(data, psi, x)    # project onto basis
recon  = spectral_reconstruct(coeffs, psi)  # reconstruct
```

### Raw operations

```python
from salt_core import (
    gradient_ratio, gradient_change, signal_to_noise,
    cf_convergents, best_convergent, is_farey_neighbor,
    jordan_product, associator, renormalized_gradient,
)

rho  = gradient_ratio(g1, g2)              # âˆˆ (0, 1)
eps  = gradient_change(g1, g2)             # relative change
c_a  = signal_to_noise(list_of_grads)      # C_Î±

convs    = cf_convergents(rho, q_max=50)   # list of (p, q)
p, q     = best_convergent(rho, eps)       # best at data resolution
adjacent = is_farey_neighbor(p1,q1,p2,q2) # |q1Â·p2 - p1Â·q2| == 1

XoW    = jordan_product(X, W)             # Â½(XW + WX), commutative
A      = associator(X, Y, Z)             # non-zero = non-associative
g_safe = renormalized_gradient(grad, alpha=1.0)  # â€–g_safeâ€– â‰¤ 1/Î±
```

---

## Demo Scenarios

```bash
python salt_demo.py                              # all 7, save to file
python salt_demo.py --scenario memorization      # noise vs signal
python salt_demo.py --scenario grokking          # phase transition detection
python salt_demo.py --scenario jordan            # commutativity, renorm
python salt_demo.py --scenario spectral          # eigenfunctions, Ford circles
python salt_demo.py --scenario convergence       # Cauchy monitor
python salt_demo.py --scenario live              # full pipeline dashboard
python salt_demo.py --scenario arithmetic        # CF convergents, Farey neighbors
python salt_demo.py --show                       # display instead of save
python salt_demo.py --outdir /tmp/salt_plots     # custom output path
```

| Scenario | What you see |
|----------|--------------|
| `memorization` | C_Î±, q*, entropy â€” diverge cleanly between noise and signal |
| `grokking` | SALT detecting the memorizationâ†’generalization transition |
| `jordan` | Commutativity proof, non-associativity, gradient renorm bound |
| `spectral` | S-L eigenfunctions, spectral reconstruction, Ford circle packing |
| `convergence` | Four parameter trajectories; Cauchy condition highlighted |
| `live` | Dashboard: 7 panels, step-by-step readout in terminal |
| `arithmetic` | CF approximation quality, Farey neighbor hit-rate in trajectories |

---

## Test Suite

```bash
python salt_tests.py
# 64 passed   0 failed   64 total
```

| Block | Tests | Covers |
|-------|-------|--------|
| Scalar observables | 11 | gradient_ratio, gradient_change, signal_to_noise |
| Continued fractions | 9 | cf_convergents, best_convergent |
| Farey arithmetic | 7 | is_farey_neighbor, word_step, path_entropy |
| FCI | 3 | farey_consolidation_index, phase_from_percentile |
| Jordan algebra | 7 | jordan_product, associator, renormalized_gradient |
| Jordan layer | 2 | JordanLayerNumpy |
| Cauchy monitor | 4 | CauchyMonitor |
| Spectral basis | 5 | sturm_liouville_eigenfunctions, spectral_features |
| SaltAnalyzer | 7 | full integration tests |
| Theorem verification | 6 | Hurwitz, commutativity, renorm bound, Farey, SL(2,â„¤), LÂ²-orthonormality |

---

## Provable Bounds

| Result | Statement |
|--------|-----------|
| Renormalized gradient | â€–âˆ‡ÌƒLâ€– â‰¤ 1/Î± uniformly; tight as â€–âˆ‡Lâ€– â†’ âˆ |
| Resolvent stability | â€–(L-Î»I)â»Â¹â€– = 1/dist(Î», Ïƒ(L)) |
| Hurwitz approximation | CF error < 1/(âˆš5Â·qÂ²) â€” sharper than 1/qÂ² |
| Cauchy convergence | {Î¸_t} Cauchy under Lipschitz loss + summable step sizes |
| LÂ² universality | Î£ aâ‚™ Ïˆâ‚™ converges to any f âˆˆ LÂ² as N â†’ âˆ |
| Jordan commutativity | Xâˆ˜W = Wâˆ˜X (immediate from definition) |

---

## Training Phenomena in SALT

| Phenomenon | SALT interpretation |
|------------|---------------------|
| **Grokking** | C_Î±(t) crosses 1 after prolonged C_Î± < 1; Farey backtrack in q* |
| **Double descent** | Interpolation threshold = C_Î± = 1 = Î»â‚ = 0 |
| **Neural collapse** | Convergence to ground eigenfunction Ï†â‚ of â„’_JL |
| **Lottery tickets** | Subnetwork whose restricted â„’_JL already has Î»â‚ > 0 at init |

---

## Open Problems

**Grokking validation (Conjecture C1).** Does q* decrease 50â€“200 steps before test accuracy rises on Power et al. (2022) modular arithmetic?

**Farey-SAM (Conjecture C4).** Under smooth loss, penalizing q* approximates penalizing Î»_max(H) with no double forward pass:
```
L_Farey(Î¸) = L(Î¸) + Î»Â·(q*)Â²
```

**Non-smooth extension.** Assumption of CÂ² loss fails at ReLU kinks. Within each linear region the CF map is well-defined; across boundaries, gradient jumps correspond to mediant insertions.

**Fisher-weighted C_Î±.** The coordinate-invariant form C_Î±^F = Î¼_g^T Fâ»Â¹ Î¼_g / Tr(Fâ»Â¹ Î£_g) is O(dÂ³) but low-rank Fisher approximations (K-FAC) may make it practical.

---

## References

| Work | Contribution to SALT |
|------|---------------------|
| Cauchy (1816) | Farey unimodular condition |
| Hurwitz (1891) | Best rational approximation bound |
| Ford (1938) | Ford circles and Farey tangency |
| Calkin & Wilf (2000) | Bijection â„³ â†’ â„šâ‚Š |
| Hardy & Wright (1979) | Continued fractions (Ch. 10â€“11) |
| Albert (1934) | Exceptional Jordan algebra Hâ‚ƒ(ğ•†) |
| Lubotzky, Phillips & Sarnak (1988) | Ramanujan graphs and optimal mixing |
| Sturm (1836), Liouville (1836) | Sturmâ€“Liouville spectral theory |
| Zettl (2005) | Modern S-L theory reference |
| Fenichel (1979) | Fastâ€“slow dynamical systems |
| McAllester (1999) | PAC-Bayes generalization bounds |
| Foret et al. (2021) | Sharpness-Aware Minimization |
| Power et al. (2022) | Grokking benchmark |
| Papyan et al. (2020) | Neural collapse |
| Li et al. (2020) | Fourier Neural Operators |
| Vigouroux et al. (2024) | Deep Sturmâ€“Liouville |
